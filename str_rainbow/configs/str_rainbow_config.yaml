model:
  name: Qwen/Qwen3-4B-Instruct-2507
  type: qwen
  temperature: 0.6
  top_p: 0.6
  max_length: 2048
  tokenizer_kwargs: {}
  model_kwargs:
    trust_remote_code: true
    device_map: auto
    low_cpu_mem_usage: true
    attn_implementation: sdpa
  dtype: bf16
  gradient_checkpointing: false

dataset:
  name: str_rainbow
  type: str_rainbow
  csv_path: ../../dataset/str_builder/data.csv
  train_split: "[:]"
  spacing: 2
  local_z: 0

output:
  base_dir: output
  save_final_model: false
  save_path: output/final_model
  verbose: false

external:
  mode: position_feedback
  original_prompt: true
  previous_response: false

magrpo:
  num_turns: 4
  num_train_epochs: 20
  per_device_train_batch_size: 1
  rollout_buffer_size: 1
  learning_rate: 5e-6
  eval_interval: 0
  eval_num_samples: 0
  num_generations: 2
  max_new_tokens: 512
  temperature: 0.6
  top_p: 0.6
  top_k: null
  joint_mode: aligned
  num_agents: 2
  discount: 0.9
  termination_threshold: -0.1
  logging_steps: 1
  save_steps: 200
  normalize_advantage: false
  epsilon_clip: null

reward_processor:
  enabled: true
  scale_factor: 1.0
  shift: -2.0

wandb:
  project: str_rainbow
  entity: OpenMLRL
  run_name: str_rainbow_magrpo
  dir: output
  tags: ["magrpo", "str_rainbow"]

prompt:
  provide_graph: false
  use_chat_template: true

task:
  block_agent1: [black_concrete, green_concrete]
  block_agent2: [white_concrete, red_concrete]
  max_commands: 300

debug:
  enabled: true
  max_prints: 1000
  every_n_calls: 5
  empty_char: "."
  raw_output: true

patches:
  generation_memory: true
  single_agent_returns: true
  force_sampling: true
